BDEPEND=app-alternatives/ninja >=dev-build/cmake-3.20.5
DEFINED_PHASES=compile configure install prepare test
DEPEND=curl? ( net-misc/curl:= ) openblas? ( sci-libs/openblas:= ) blis? ( sci-libs/blis:= ) hip? ( >=dev-util/hip-6.3:= )
DESCRIPTION=Port of Facebook's LLaMA model in C/C++
EAPI=8
HOMEPAGE=https://github.com/ggerganov/llama.cpp
INHERIT=cmake rocm
IUSE=curl openblas blis hip +amdgpu_targets_gfx906 +amdgpu_targets_gfx908 +amdgpu_targets_gfx90a +amdgpu_targets_gfx942 +amdgpu_targets_gfx1030 +amdgpu_targets_gfx1100 amdgpu_targets_gfx803 amdgpu_targets_gfx900 amdgpu_targets_gfx940 amdgpu_targets_gfx941 amdgpu_targets_gfx1010 amdgpu_targets_gfx1011 amdgpu_targets_gfx1012 amdgpu_targets_gfx1031 amdgpu_targets_gfx1101 amdgpu_targets_gfx1102
KEYWORDS=~amd64
LICENSE=MIT
RDEPEND=curl? ( net-misc/curl:= ) openblas? ( sci-libs/openblas:= ) blis? ( sci-libs/blis:= ) hip? ( >=dev-util/hip-6.3:= ) dev-python/numpy
REQUIRED_USE=?? ( openblas blis )
SLOT=0
SRC_URI=https://github.com/ggerganov/llama.cpp/archive/refs/tags/b5097.tar.gz -> llama-cpp-0_pre5097.tar.gz
_eclasses_=toolchain-funcs	f9d71a6efe9d083aec750dd13968e169	flag-o-matic	b892042b2667b8ac69ec8a2571dc290a	multiprocessing	1e32df7deee68372153dca65f4a7c21f	ninja-utils	2df4e452cea39a9ec8fb543ce059f8d6	xdg-utils	42869b3c8d86a70ef3cf75165a395e09	cmake	90d8ee8393c5c815637fd3cb87828c9b	rocm	826765f795a41b937d1bfe8e709346cd
_md5_=1d9c0299b03b67d9071dd9f21be5ab7d
